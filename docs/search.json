[
  {
    "objectID": "posts/09.Regression.html",
    "href": "posts/09.Regression.html",
    "title": "9. Regression",
    "section": "",
    "text": "1. imports\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n\n\n2. data\n\ndata(diamonds)\n\n- cut, color, clarity를 범주형(factor)로 변환\n\ndiamonds$cut &lt;- factor(diamonds$cut, ordered = FALSE)\ndiamonds$color &lt;- factor(diamonds$color, ordered = FALSE)\ndiamonds$clarity &lt;- factor(diamonds$clarity, ordered = FALSE)\n\n- 사용할 변수 선택\n\ndiamonds_sub &lt;- diamonds[, c(\"price\", \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"x\", \"y\", \"z\")]\n\n- 데이터 셔플\n\nset.seed(2025)  # 재현성 확보\nshuffled_idx &lt;- sample(nrow(diamonds_sub))\ndiamonds_sub &lt;- diamonds_sub[shuffled_idx, ]\n\n- 데이터 분할(70% train, 30% test)\n\nn &lt;- nrow(diamonds_sub)\nn_train &lt;- round(0.7 * n)\n\n\ntrain_data &lt;- diamonds_sub[1:n_train, ]\ntest_data  &lt;- diamonds_sub[(n_train + 1):n, ]\n\n\n\n3. 모델 학습\n\nmodel &lt;- lm(price ~ carat + cut + color + clarity + depth + table + x + y + z, data = train_data)\n\n\nsummary(model)\n\n\nCall:\nlm(formula = price ~ carat + cut + color + clarity + depth + \n    table + x + y + z, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21193.1   -592.3   -184.2    379.4  10019.2 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2523.821    486.284   5.190 2.11e-07 ***\ncarat        11166.722     57.804 193.183  &lt; 2e-16 ***\ncutGood        569.597     40.501  14.064  &lt; 2e-16 ***\ncutVery Good   708.343     38.926  18.197  &lt; 2e-16 ***\ncutPremium     752.076     38.947  19.310  &lt; 2e-16 ***\ncutIdeal       810.282     40.340  20.086  &lt; 2e-16 ***\ncolorE        -207.868     21.473  -9.681  &lt; 2e-16 ***\ncolorF        -253.152     21.671 -11.681  &lt; 2e-16 ***\ncolorG        -476.298     21.273 -22.390  &lt; 2e-16 ***\ncolorH        -967.243     22.517 -42.956  &lt; 2e-16 ***\ncolorI       -1462.027     25.326 -57.728  &lt; 2e-16 ***\ncolorJ       -2379.519     31.042 -76.655  &lt; 2e-16 ***\nclaritySI2    2660.652     51.379  51.784  &lt; 2e-16 ***\nclaritySI1    3638.154     51.113  71.179  &lt; 2e-16 ***\nclarityVS2    4247.626     51.407  82.627  &lt; 2e-16 ***\nclarityVS1    4551.296     52.258  87.093  &lt; 2e-16 ***\nclarityVVS2   4918.192     53.839  91.350  &lt; 2e-16 ***\nclarityVVS1   4987.707     55.359  90.097  &lt; 2e-16 ***\nclarityIF     5297.251     59.893  88.445  &lt; 2e-16 ***\ndepth          -69.236      5.335 -12.979  &lt; 2e-16 ***\ntable          -28.518      3.496  -8.157 3.55e-16 ***\nx             -964.321     35.990 -26.794  &lt; 2e-16 ***\ny               -2.101     19.444  -0.108    0.914    \nz              -40.404     35.011  -1.154    0.248    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1129 on 37734 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9197 \nF-statistic: 1.881e+04 on 23 and 37734 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n4. 모델 평가\n- test_set 예측\n\ntest_pred &lt;- predict(model, newdata = test_data)\n\n- test 성능 평가(RMSE 계산)\n\ntest_rmse &lt;- sqrt(mean((test_pred - test_data$price)^2))\ncat(\"Test RMSE:\", round(test_rmse, 2), \"\\n\")\n\nTest RMSE: 1132.54 \n\n\n- 예측 vs 실제 산점도 (test set)\n\nplot(test_pred, test_data$price,\n     xlab = \"Predicted Price\", ylab = \"Actual Price\",\n     main = \"Test Set: Predicted vs Actual Price\",\n     pch = 16, col = \"blue\")\nabline(0, 1, col = \"red\", lwd = 2)  # 완벽한 예측선 (y = x)\n\n\n\n\n\n\n\n\n- RMSE값 그래프에 추가\n\ntext(x = max(test_pred) * 0.7, \n     y = min(test_data$price) * 1.2, \n     labels = paste0(\"Test RMSE = \", round(test_rmse, 2)),\n     pos = 4, \n     col = \"black\", \n     cex = 0.9)\n\nERROR: Error in text.default(x = max(test_pred) * 0.7, y = min(test_data$price) * : plot.new has not been called yet\n\nError in text.default(x = max(test_pred) * 0.7, y = min(test_data$price) * : plot.new has not been called yet\nTraceback:\n\n1. text(x = max(test_pred) * 0.7, y = min(test_data$price) * 1.2, \n .     labels = paste0(\"Test RMSE = \", round(test_rmse, 2)), pos = 4, \n .     col = \"black\", cex = 0.9)\n2. text.default(x = max(test_pred) * 0.7, y = min(test_data$price) * \n .     1.2, labels = paste0(\"Test RMSE = \", round(test_rmse, 2)), \n .     pos = 4, col = \"black\", cex = 0.9)\n\n\n- 새로운 데이터 생성\n\nnew_data &lt;- data.frame(\n  carat = c(0.5, 1.2),\n  cut = factor(c(\"Ideal\", \"Premium\"), levels = levels(diamonds$cut)),\n  color = factor(c(\"E\", \"H\"), levels = levels(diamonds$color)),\n  clarity = factor(c(\"VS2\", \"SI1\"), levels = levels(diamonds$clarity)),\n  depth = c(61.5, 62.2),\n  table = c(55, 58),\n  x = c(5.1, 6.9),\n  y = c(5.2, 7.1),\n  z = c(3.15, 4.35)\n)\n\n- 새로운 데이터 예측 및 결과 출력\n\npredicted_prices &lt;- predict(model, newdata = new_data)\n\n\nnew_data$predicted_price &lt;- round(predicted_prices, 2)\nnew_data\n\n\nA data.frame: 2 × 10\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nx\ny\nz\npredicted_price\n\n\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0.5\nIdeal\nE\nVS2\n61.5\n55\n5.1\n5.2\n3.15\n2074.45\n\n\n1.2\nPremium\nH\nSI1\n62.2\n58\n6.9\n7.1\n4.35\n6541.83"
  },
  {
    "objectID": "posts/06.Prediction_Simulations1.html",
    "href": "posts/06.Prediction_Simulations1.html",
    "title": "6. Prediction Simulations 1",
    "section": "",
    "text": "# Set seed for reproducibility\nset.seed(123)\n\n# Number of repetitions for the entire process\nn_repetitions &lt;- 50\n\n# Number of data points in training and test sets\nn_samples &lt;- 100\n\n# Parameters for the Normal distribution\ntrue_mean &lt;- 3\ntrue_variance &lt;- 9\ntrue_sd &lt;- sqrt(true_variance) # rnorm uses standard deviation\n\n\n# Vector to store the Mean Squared Sum for training, test data for each repetition\ntrain_mse_results &lt;- numeric(n_repetitions)\ntest_mse_results &lt;- numeric(n_repetitions)\n\n\n# --- Start the Simulation Loop ---\n\nfor (i in 1:n_repetitions) {\n\n  # 1. Generate Training Data\n  train_data &lt;- rnorm(n_samples, mean = true_mean, sd = true_sd)\n\n  # 2. Calculate the Sample Mean from Training Data\n  train_sample_mean &lt;- mean(train_data)\n\n  # 3. Calculate MSE on Training Data\n  train_squared_diff &lt;- (train_data - train_sample_mean)^2\n  train_mse &lt;- mean(train_squared_diff)\n  train_mse_results[i] &lt;- train_mse\n\n  # 4. Generate Test Data (Evaluation Data)\n  test_data &lt;- rnorm(n_samples, mean = true_mean, sd = true_sd)\n\n  # 5. Calculate MSE on Test Data using the Training Sample Mean\n  test_squared_diff &lt;- (test_data - train_sample_mean)^2\n  test_mse &lt;- mean(test_squared_diff)\n  test_mse_results[i] &lt;- test_mse\n\n} # End of the simulation loop\n\n\n# --- Visualize the Results ---\n\nplot(train_mse_results, test_mse_results,\n     main = \"Comparison of Training MSE and Test MSE (50 Repetitions)\",\n     xlab = \"MSE on Training Data\",\n     ylab = \"MSE on Test Data\",\n     pch = 19,\n     col = \"blue\")\n\n# Add a diagonal line (y=x) for reference\nabline(a = 0, b = 1, col = \"red\", lty = 2) # lty=2 makes the line dashed\n\n# Add a legend\nlegend(\"topleft\", legend = \"y = x line\", col = \"red\", lty = 2)"
  },
  {
    "objectID": "posts/07.Prediction_Simulations2.html",
    "href": "posts/07.Prediction_Simulations2.html",
    "title": "7. Prediction Simulations 2",
    "section": "",
    "text": "# Number of simulations, samples\nn_simulations &lt;- 500\nn_train &lt;- 100\nn_test &lt;- 100\n\n# Number of predictor variables\nn_vars_total &lt;- 10\n# Number of predictor variables with non-zero true coefficients\nn_vars_true &lt;- 2\n# Number of noise variables (total - true)\nn_vars_noise &lt;- n_vars_total - n_vars_true\n\n# True regression coefficients\ntrue_beta &lt;- c(5, -3, rep(0, n_vars_noise))\n\n# True intercept\ntrue_intercept &lt;- 2\n\n# Standard deviation of the error term (noise in the Y value)\nerror_sd &lt;- 3\n\n# --- Initialization ---\nrmse_train_all &lt;- numeric(n_simulations) \nrmse_test_all &lt;- numeric(n_simulations) \n\nrmse_train_true &lt;- numeric(n_simulations) \nrmse_test_true &lt;- numeric(n_simulations)  \n\nrmse_train_plus1 &lt;- numeric(n_simulations) \nrmse_test_plus1 &lt;- numeric(n_simulations) \n\n\n# Set seed for reproducibility\nset.seed(1015)\n\n# --- Simulation Loop ---\n\n# Loop 50 times\nfor (i in 1:n_simulations) {\n  \n  # Generate predictor variables (X) from a standard normal distribution\n  X_train_matrix &lt;- matrix(rnorm(n_train * n_vars_total), nrow = n_train, ncol = n_vars_total)\n  # Generate the error term (epsilon)\n  epsilon_train &lt;- rnorm(n_train, mean = 0, sd = error_sd)\n  # Calculate the response variable (Y) using the true model: Y = intercept + X * beta + epsilon\n  Y_train_vector &lt;- true_intercept + X_train_matrix %*% true_beta + epsilon_train\n  # Combine predictors and response into a data frame (useful for lm function)\n  train_df &lt;- data.frame(Y = Y_train_vector, X_train_matrix)\n  # Assign meaningful column names\n  colnames(train_df) &lt;- c(\"Y\", paste0(\"X\", 1:n_vars_total))\n  \n  # Generate independent test data using the same process\n  X_test_matrix &lt;- matrix(rnorm(n_test * n_vars_total), nrow = n_test, ncol = n_vars_total)\n  epsilon_test &lt;- rnorm(n_test, mean = 0, sd = error_sd)\n  Y_test_vector &lt;- true_intercept + X_test_matrix %*% true_beta + epsilon_test\n  test_df &lt;- data.frame(Y = Y_test_vector, X_test_matrix)\n  colnames(test_df) &lt;- c(\"Y\", paste0(\"X\", 1:n_vars_total))\n  \n  # Model 1: Using all predictor variables\n  model_all &lt;- lm(Y ~ ., data = train_df)\n  \n  # Model 2: Using only the true predictor variables (X1 and X2 in this case)\n  true_var_names &lt;- paste0(\"X\", 1:n_vars_true)\n  formula_true_str &lt;- paste(\"Y ~\", paste(true_var_names, collapse = \" + \")) \n  formula_true &lt;- as.formula(formula_true_str)\n  model_true &lt;- lm(formula_true, data = train_df)\n  \n  # Model 3: Using true predictors + 1 randomly chosen noise predictor\n  noise_var_indices &lt;- (n_vars_true + 1):n_vars_total\n  chosen_noise_index &lt;- sample(noise_var_indices, 1)\n  chosen_noise_var_name &lt;- paste0(\"X\", chosen_noise_index)\n  vars_plus1 &lt;- c(true_var_names, chosen_noise_var_name)\n  formula_plus1_str &lt;- paste(\"Y ~\", paste(vars_plus1, collapse = \" + \")) # Create formula string\n  formula_plus1 &lt;- as.formula(formula_plus1_str) # Convert to formula\n  model_plus1 &lt;- lm(formula_plus1, data = train_df)\n  \n  # Calculate fitted values using the estimated model on the original training data\n  fitted_train_all &lt;- predict(model_all, newdata = train_df)\n  fitted_train_true &lt;- predict(model_true, newdata = train_df)\n  fitted_train_plus1 &lt;- predict(model_plus1, newdata = train_df)\n  \n  # Make predictions using the estimated model on the unseen evaluation data\n  pred_test_all &lt;- predict(model_all, newdata = eval_df)\n  pred_test_true &lt;- predict(model_true, newdata = eval_df)\n  pred_test_plus1 &lt;- predict(model_plus1, newdata = eval_df)\n  \n  # RMSE on training data (using fitted values) - Measures goodness of fit to the training data\n  rmse_train_all[i] &lt;- sqrt(mean((train_df$Y - fitted_train_all)^2))\n  rmse_train_true[i] &lt;- sqrt(mean((train_df$Y - fitted_train_true)^2))\n  rmse_train_plus1[i] &lt;- sqrt(mean((train_df$Y - fitted_train_plus1)^2))\n  \n  # RMSE on evaluation data (using predicted values) - Measures prediction accuracy on unseen data\n  rmse_test_all[i] &lt;- sqrt(mean((eval_df$Y - pred_test_all)^2))\n  rmse_test_true[i] &lt;- sqrt(mean((eval_df$Y - pred_test_true)^2))\n  rmse_test_plus1[i] &lt;- sqrt(mean((eval_df$Y - pred_test_plus1)^2))\n\n} # End of the simulation loop\n\nERROR: Error in eval(expr, envir, enclos): object 'eval_df' not found\n\nError in eval(expr, envir, enclos): object 'eval_df' not found\nTraceback:\n\n1. predict(model_all, newdata = eval_df)\n2. predict.lm(model_all, newdata = eval_df)\n\n\n\n\n# --- Step 6: Create Scatter Plots ---\npar(mfrow = c(1, 3), mar = c(4, 4, 3, 1), oma = c(0, 0, 2, 0)) # Adjust margins\n\n# Determine common axis limits for better comparison across plots\nall_rmse_values &lt;- c(rmse_train_all, rmse_test_all,\n                     rmse_train_true, rmse_test_true,\n                     rmse_train_plus1, rmse_test_plus1)\nplot_lim &lt;- range(all_rmse_values, na.rm = TRUE)\n\n# Plot 1: All Predictors\nplot(x = rmse_train_all, y = rmse_test_all,\n     main = \"Model: All Predictors\",\n     xlab = \"Training RMSE (Fit)\",       \n     ylab = \"Evaluation RMSE (Predict)\",  \n     pch = 19, col = \"dodgerblue\",\n     xlim = plot_lim, # Use common limits\n     ylim = plot_lim)\nabline(a = 0, b = 1, col = \"red\", lty = 2) # y=x line\n\n# Plot 2: True Predictors Only\nplot(x = rmse_train_true, y = rmse_test_true,\n     main = \"Model: True Predictors\",\n     xlab = \"Training RMSE\",\n     ylab = \"Test RMSE\",\n     pch = 19, col = \"forestgreen\",\n     xlim = plot_lim,\n     ylim = plot_lim)\nabline(a = 0, b = 1, col = \"red\", lty = 2)\n\n# Plot 3: True + 1 Noise Predictor\nplot(x = rmse_train_plus1, y = rmse_test_plus1,\n     main = \"Model: True + 1 Noise\",\n     xlab = \"Training RMSE (Fit)\",\n     ylab = \"Evaluation RMSE (Predict)\",\n     pch = 19, col = \"darkorange\",\n     xlim = plot_lim,\n     ylim = plot_lim)\nabline(a = 0, b = 1, col = \"red\", lty = 2)\n\n# Add an overall title to the figure\nmtext(\"Training RMSE (Goodness of Fit) vs. Evaluation RMSE (Prediction Accuracy)\", outer = TRUE, cex = 1.2) # More descriptive title\n\n# Reset plotting parameters to default (1 plot per device)\npar(mfrow = c(1, 1), mar = c(5.1, 4.1, 4.1, 2.1), oma = c(0, 0, 0, 0))"
  },
  {
    "objectID": "posts/01.R_distributions.html",
    "href": "posts/01.R_distributions.html",
    "title": "1. R distributions",
    "section": "",
    "text": "1. Normal Distribution(정규분포)\n- (Random Number Generation)\n- rnorm(n, mean, sd) -&gt; 평균 mean, 표준편차 sd 에서 n개 랜덤 추출\n\nnormal_random &lt;- rnorm(n = 10, mean = 0, sd = 1)\nprint(\"Normal Random Numbers:\")\nprint(normal_random)\n\n[1] \"Normal Random Numbers:\"\n [1] -0.72730915 -0.09955575 -2.01791946 -1.17899047  0.08952181  0.84630774\n [7]  0.34269490  0.17711561  1.02601457  0.60893424\n\n\n- (Probability Density Function - PDF)\n\n확률밀도함수\n\n\nnormal_pdf &lt;- dnorm(x = seq(-10,10,by=0.05), mean = 0, sd = 1)\nplot(seq(-10,10,by=0.05),normal_pdf, type='l', main ='Normal PDF')\n\n\n\n\n\n\n\n\n- (Cumulative Distribution Function - CDF)\n\n누적분포함수\n\n\nnormal_cdf &lt;- pnorm(q = seq(-10,10,by=0.05), mean = 0, sd = 1)\nplot(seq(-10,10,by=0.05),normal_cdf, main='Norma CDF', type='l' )\n\n\n\n\n\n\n\n\n- (Quantile Function)\n\n분위함수\n\n\nnormal_quantile &lt;- qnorm(p = 0.95, mean = 0, sd = 1)  # 95% 분위수\nprint(\"Normal Quantile at p=0.95:\")\nprint(normal_quantile)\n\n[1] \"Normal Quantile at p=0.95:\"\n[1] 1.644854\n\n\n\n\n2. Bernoulli Distribution(베르누이 분포)\n- rbinom\n\np=prob 에서\nsize번 베르누이 시행해서 성공한 횟수\nn번 반복해서 return\n\n\nbernoulli_random &lt;- rbinom(n = 10, size = 1, prob = 0.6)\nprint(\"Bernoulli Random Numbers:\")\nprint(bernoulli_random)\n\n[1] \"Bernoulli Random Numbers:\"\n [1] 1 1 1 0 0 1 1 0 1 1\n\n\n- (Probability Mass Function - PMF)\n\n확률 질량 함수\n\n\nbernoulli_pmf &lt;- dbinom(x = c(0,1), size = 1, prob = 0.6)\nbarplot(bernoulli_pmf, main ='Bernoulli pmf')\n\n\n\n\n\n\n\n\n- pbinom(q, size, prob)\n\n누적 분포 함수\n\n\nbernoulli_cdf &lt;- pbinom(q = c(0,1), size = 1, prob = 0.6)\nplot(bernoulli_cdf~c(0,1), xlim = c(-0.2, 1.2),main ='Bernoulli cdf', type='s')\n\n\n\n\n\n\n\n\n\nbernoulli_cdf &lt;- pbinom(q = c(0,1,2,3), size = 3, prob = 0.6)\nplot(bernoulli_cdf~c(0,1,2,3), xlim = c(-0.2, 3.2),main ='Bernoulli cdf', type='s')\n\n\n\n\n\n\n\n\n- (Quantile Function)\n\n분위함수\n\n\nbernoulli_quantile &lt;- qbinom(p = 0.8, size = 1, prob = 0.6)\nprint(\"Bernoulli Quantile at p=0.8:\")\nprint(bernoulli_quantile)\n\n[1] \"Bernoulli Quantile at p=0.8:\"\n[1] 1\n\n\n\n\n3. Binomial Distribution(이항분포)\n- 베르누이분포와 같은 함수지만 size를 1이 아니라 B(n,p)에서의 n으로 바꿈\n\nbinomial_random &lt;- rbinom(n = 10, size = 5, prob = 0.4)\nprint(\"Binomial Random Numbers:\")\nprint(binomial_random)\n\n[1] \"Binomial Random Numbers:\"\n [1] 2 3 3 2 3 1 1 3 3 1\n\n\n- 확률 질량 함수\n\ndbinom(x, size, prob)\n\n\nbernoulli_pmf &lt;- dbinom(x = c(0:5), size = 5, prob = 0.4)\nbarplot(bernoulli_pmf, main ='Binomial pmf')\n# 0,1,2,3,4,5 가 나올 확률\n\n\n\n\n\n\n\n\n- 이항분포의 특성상 분산이 더 작음\n\nE(X) = np\nVar(X) = npq\nq=1-p 이므로 &lt;=1\n\n\nbinomial_random &lt;- rbinom(n = 100, size = 5, prob = 0.4)\nmean(binomial_random)\nvar(binomial_random)\n\n2.21\n\n\n1.42010101010101\n\n\n- 누적 분포 함수\n\npbinom(q, size, prob)\n\n\nbinomial_cdf &lt;- pbinom(q = c(0:5), size = 5, prob = 0.4)\nplot(binomial_cdf~c(0:5), main ='Binomial cdf', xlim=c(0,5), type='s')\n\n\n\n\n\n\n\n\n- (Quantile Function)\n\n분위함수\nqbinom(p, size, prob)\n\n\nbinomial_quantile &lt;- qbinom(p = 0.7, size = 5, prob = 0.4)\nprint(\"Binomial Quantile at p=0.7:\")\nprint(binomial_quantile)\n\n[1] \"Binomial Quantile at p=0.7:\"\n[1] 3\n\n\n\n\n4. Exponential Distribution(지수분포)\n- rexp(n, rate)\n\nexponential_random &lt;- rexp(n = 10, rate = 2)\nprint(\"Exponential Random Numbers:\")\nprint(exponential_random)\n\n[1] \"Exponential Random Numbers:\"\n [1] 0.1571845 0.1074697 0.5828041 0.3155801 1.0756425 0.2600387 0.1384434\n [8] 1.6451927 0.1802277 0.3058570\n\n\n\nmean(rexp(n = 10, rate = 2))\nmean(rexp(n = 100000, rate = 10))\nmean(rexp(n = 100000, rate = 10))\n#거의 1에 근사하게 나옴\n\n0.82651321556108\n\n\n0.0997617675944707\n\n\n0.100401256810418\n\n\n- 확률 밀도 함수\n\ndexp(x, rate)\n\n\nexponential_pdf &lt;- dexp(x = seq(0, 10, length=1000), \n                        rate = 2)\nplot(exponential_pdf~c(seq(0, 10, length=1000)), \n      main=\"Exponential PDF\", type='l', xlab='x', ylab='value')\n\n\n\n\n\n\n\n\n- 누적 분포 함수\n\npexp(q, rate)\n\n\nexponential_cdf &lt;- pexp(q = seq(0, 10, length=1000), rate = 2)\nplot(exponential_cdf~c(seq(0, 10, length=1000)), \n     main=\"Exponential CDF\", type='s', xlab='x', ylab='value')\n###print(exponential_cdf)\n\n\n\n\n\n\n\n\n- (Quantile Function)\n\n분위함수\nqexp(p, rate)\n\n\nexponential_quantile &lt;- qexp(p = 0.6, rate = 2)\nprint(\"Exponential Quantile at p=0.6:\")\nprint(exponential_quantile)\n\n[1] \"Exponential Quantile at p=0.6:\"\n[1] 0.4581454\n\n\n\n\n5. Poisson Distribution(포아송 분포)\n- rpois\n\nlambda=3의 포아송 \\(\\to\\) 결과\nn=10번 반복\n\n\npoisson_random &lt;- rpois(n = 10, lambda = 3)\nprint(\"Poisson Random Numbers:\")\nprint(poisson_random)\n\n[1] \"Poisson Random Numbers:\"\n [1] 1 4 2 5 4 3 1 4 2 5\n\n\n포아송 분포를 따르는 변수의 평균과 분산이 같음\n\nmean(rpois(n=100000,lambda=3))\nvar(rpois(n=100000,lambda=3))\n\n2.99803\n\n\n2.97500698366984\n\n\n- 확률 질량 함수\n\ndpois(x,lambda)\n\n\npoisson_pmf &lt;- dpois(x = seq(0,12,1), lambda = 3)\nnames(poisson_pmf) = seq(0,12,1)\nbarplot(poisson_pmf, main=\"Poisson PMF\")\n\n\n\n\n\n\n\n\n- 누적 분포 함수\n\nppois(q, lambda)\n\n\npoisson_cdf &lt;- ppois(q = seq(0,12,1), lambda = 3)\nnames(poisson_pmf) = seq(0,12,1)\nplot(poisson_cdf~c(0:12), main=\"Poisson CDF\", xlab='x', \n    ylab='value', type='s')\n\n\n\n\n\n\n\n\n- (Quantile Function)\n\n분위함수\nqpois(p, lambda)\n\n\npoisson_quantile &lt;- qpois(p = 0.9, lambda = 3)\nprint(\"Poisson Quantile at p=0.9:\")\nprint(poisson_quantile)\n\n[1] \"Poisson Quantile at p=0.9:\"\n[1] 5\n\n\n\n- 추가\n- 감마 분포\n\n평균 : \\(\\alpha\\) x \\(\\beta\\)\n분산 : \\(\\alpha\\) x \\(\\beta^2\\)\n\n\nmean(rgamma(100000,1,3))\nvar(rgamma(100000,1,3))\n\n0.333018431858545\n\n\n0.111819570741366\n\n\n\n- ggplot\n\nlibrary(ggplot2)\nlibrary(tidyr) \n\n\nresults &lt;- data.frame(\n  Distribution = character(),\n  Value = numeric(),\n  Type = character(),  # Mean, Median, Variance를 구분하는 열 추가\n  stringsAsFactors = FALSE\n)\n\n\n# 1. Normal Distribution\nnormal_data &lt;- rnorm(n = 1000, mean = 0, sd = 1)\n\n\n# ggplot histogram \nggplot(data.frame(Value = normal_data), aes(x = Value)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +  # bins: 막대 개수\n  labs(title = \"Normal Distribution Histogram\", x = \"Value\", y = \"Frequency\") +\n  theme_bw() \n\n\n\n\n\n\n\n\n\n# results \nresults &lt;- rbind(results,\n                 data.frame(Distribution = \"Normal\", Value = mean(normal_data), Type = \"Mean\"),\n                 data.frame(Distribution = \"Normal\", Value = median(normal_data), Type = \"Median\"),\n                 data.frame(Distribution = \"Normal\", Value = var(normal_data), Type = \"Variance\"))\n\n\n# 2. Bernoulli Distribution\nbernoulli_data &lt;- rbinom(n = 1000, size = 1, prob = 0.7)\n\n\n# ggplot bar plot \nggplot(data.frame(Outcome = factor(bernoulli_data)), aes(x = Outcome)) +\n  geom_bar(fill = \"coral\", color = \"black\") +\n  labs(title = \"Bernoulli Distribution Bar Plot\", x = \"Outcome (0: Failure, 1: Success)\", y = \"Frequency\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nresults &lt;- rbind(results,\n                 data.frame(Distribution = \"Bernoulli\", Value = mean(bernoulli_data), Type = \"Mean\"),\n                 data.frame(Distribution = \"Bernoulli\", Value = median(bernoulli_data), Type = \"Median\"),\n                 data.frame(Distribution = \"Bernoulli\", Value = var(bernoulli_data), Type = \"Variance\"))\n\n\n# 3. Binomial Distribution\nbinomial_data &lt;- rbinom(n = 1000, size = 10, prob = 0.3)\n\n\n# ggplot histogram \nggplot(data.frame(Successes = binomial_data), aes(x = Successes)) +\n  geom_histogram(binwidth = 1, fill = \"lightgreen\", color = \"black\") +  # binwidth: 막대 너비\n  labs(title = \"Binomial Distribution Histogram\", x = \"Number of Successes\", y = \"Frequency\") +\n  scale_x_continuous(breaks = seq(0, 10, by = 1)) +  # x축 눈금 설정\n  theme_bw()\n\n\n\n\n\n\n\n\n\nresults &lt;- rbind(results,\n                 data.frame(Distribution = \"Binomial\", Value = mean(binomial_data), Type = \"Mean\"),\n                 data.frame(Distribution = \"Binomial\", Value = median(binomial_data), Type = \"Median\"),\n                 data.frame(Distribution = \"Binomial\", Value = var(binomial_data), Type = \"Variance\"))\n\n\n# 4. Exponential Distribution\nexponential_data &lt;- rexp(n = 1000, rate = 2)\n\n\n# ggplot histogram \nggplot(data.frame(Time = exponential_data), aes(x = Time)) +\n  geom_histogram(bins = 30, fill = \"gold\", color = \"black\") +\n  labs(title = \"Exponential Distribution Histogram\", x = \"Time\", y = \"Frequency\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nresults &lt;- rbind(results,\n                 data.frame(Distribution = \"Exponential\", Value = mean(exponential_data), Type = \"Mean\"),\n                 data.frame(Distribution = \"Exponential\", Value = median(exponential_data), Type = \"Median\"),\n                 data.frame(Distribution = \"Exponential\", Value = var(exponential_data), Type = \"Variance\"))\n\n\n# 5. Poisson Distribution\npoisson_data &lt;- rpois(n = 1000, lambda = 5)\n\n\n# ggplot histogram \nggplot(data.frame(Events = poisson_data), aes(x = Events)) +\n  geom_histogram(binwidth = 1, fill = \"violet\", color = \"black\") +\n  labs(title = \"Poisson Distribution Histogram\", x = \"Number of Events\", y = \"Frequency\") +\n  scale_x_continuous(breaks = seq(0, max(poisson_data), by = 1)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nresults &lt;- rbind(results,\n                 data.frame(Distribution = \"Poisson\", Value = mean(poisson_data), Type = \"Mean\"),\n                 data.frame(Distribution = \"Poisson\", Value = median(poisson_data), Type = \"Median\"),\n                 data.frame(Distribution = \"Poisson\", Value = var(poisson_data), Type = \"Variance\"))\n\n\nresults\n\n\nA data.frame: 15 × 3\n\n\nDistribution\nValue\nType\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\nNormal\n0.01671411\nMean\n\n\nNormal\n0.01257944\nMedian\n\n\nNormal\n0.92430136\nVariance\n\n\nBernoulli\n0.70800000\nMean\n\n\nBernoulli\n1.00000000\nMedian\n\n\nBernoulli\n0.20694294\nVariance\n\n\nBinomial\n2.94600000\nMean\n\n\nBinomial\n3.00000000\nMedian\n\n\nBinomial\n2.10919319\nVariance\n\n\nExponential\n0.51712705\nMean\n\n\nExponential\n0.34010939\nMedian\n\n\nExponential\n0.28038007\nVariance\n\n\nPoisson\n4.93600000\nMean\n\n\nPoisson\n5.00000000\nMedian\n\n\nPoisson\n4.74464865\nVariance"
  },
  {
    "objectID": "posts/08.Classifier.html",
    "href": "posts/08.Classifier.html",
    "title": "8. Classifier",
    "section": "",
    "text": "1. library\n\n# install.packages(c(\"palmerpenguins\", \"class\", \"caret\")) # Run this line once if packages are not installed\nlibrary(palmerpenguins)\nlibrary(class) # For k-NN\nlibrary(caret) # For confusionMatrix and createDataPartition\n\nLoading required package: ggplot2\n\nLoading required package: lattice\n\nWarning message:\n“Your system is mis-configured: ‘/var/db/timezone/localtime’ is not a symlink”\nWarning message:\n“‘/var/db/timezone/localtime’ is not identical to any known timezone file”\n\n\n\n\n2. Data\n\n# --- 1. Data Preparation ---\ndata(penguins)\npenguins_complete &lt;- na.omit(penguins)\nselected_data &lt;- penguins_complete[, c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"year\", \"sex\", \"species\")]\nset.seed(123)\n\n- train, test 분리\n\n# Split data into training (70%) and testing (30%) sets\ntrain_index &lt;- createDataPartition(selected_data$species, p = 0.7, list = FALSE)\ntraining_data &lt;- selected_data[train_index, ]\ntesting_data &lt;- selected_data[-train_index, ]\n\n\n\n3. Logistic Regression Classifier(성별(sex) 예측)\n\n# --- 2. Logistic Regression Classifier (Predicting Sex) ---\nlg_predictor_variables &lt;- c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\")\nlg_target_variable &lt;- \"sex\"\n\nlogistic_model &lt;- glm(\n  formula = paste(lg_target_variable, \"~\", paste(lg_predictor_variables, collapse = \" + \")),\n  data = training_data,\n  family = binomial(link = \"logit\") # Specify logistic regression\n)\n\n\nlogistic_probabilities &lt;- predict(logistic_model, newdata = testing_data, type = \"response\")\nclassified_sex &lt;- ifelse(logistic_probabilities &gt; 0.5, \"male\", \"female\")\nclassified_sex &lt;- factor(classified_sex, levels = levels(testing_data$sex))\n\n\n\n4. Logistic Regression 평가\n\nlg_conf_matrix_details &lt;- confusionMatrix(\n  data = classified_sex,                   # Classified classes\n  reference = testing_data[[lg_target_variable]] # Actual classes\n)\nlg_conf_matrix_details$table\nlg_conf_matrix_details$overall[\"Accuracy\"]\nlg_conf_matrix_details$byClass[c(\"Precision\", \"Recall\", \"Sensitivity\", \"Specificity\", \"Pos Pred Value\", \"Neg Pred Value\")]\n\n          Reference\nPrediction female male\n    female     42    4\n    male        6   46\n\n\nAccuracy: 0.897959183673469\n\n\nPrecision0.91304347826087Recall0.875Sensitivity0.875Specificity0.92Pos Pred Value0.91304347826087Neg Pred Value0.884615384615385\n\n\n\n\n5. k-Nearest Neighbor (k-NN) Classifier\n\nknn_predictor_variables &lt;- c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"year\")\nknn_target_variable &lt;- \"species\"\ntraining_data_knn &lt;- training_data\ntesting_data_knn &lt;- testing_data\ntraining_data_knn[knn_predictor_variables] &lt;- scale(training_data_knn[knn_predictor_variables])\ntesting_data_knn[knn_predictor_variables] &lt;- scale(testing_data_knn[knn_predictor_variables],\n                                                   center = attr(scale(training_data[knn_predictor_variables]), \"scaled:center\"),\n                                                   scale = attr(scale(training_data[knn_predictor_variables]), \"scaled:scale\"))\n\nknn_classification &lt;- knn(\n  train = training_data_knn[, knn_predictor_variables], # Training predictors (scaled)\n  test = testing_data_knn[, knn_predictor_variables],   # Testing predictors (scaled)\n  cl = training_data[[knn_target_variable]],            # True class labels for training data\n  k = 5\n)\n\n\n\n6. k-Nearest Neightbor 평가\n\nknn_conf_matrix_details &lt;- confusionMatrix(\n  data = knn_classification,                      # classified classes\n  reference = testing_data[[knn_target_variable]] # Actual classes\n)\nknn_conf_matrix_details$table\nknn_conf_matrix_details$overall[\"Accuracy\"]\nknn_conf_matrix_details$byClass[, c(\"Precision\", \"Recall\")]\n\n           Reference\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        43         0      0\n  Chinstrap      0        20      0\n  Gentoo         0         0     35\n\n\nAccuracy: 1\n\n\n\nA matrix: 3 × 2 of type dbl\n\n\n\nPrecision\nRecall\n\n\n\n\nClass: Adelie\n1\n1\n\n\nClass: Chinstrap\n1\n1\n\n\nClass: Gentoo\n1\n1"
  },
  {
    "objectID": "posts/10.K-NN_Regression.html",
    "href": "posts/10.K-NN_Regression.html",
    "title": "10. K-NN Regression",
    "section": "",
    "text": "1. library\n\ninstall.packages(\"FNN\")\n\nUpdating HTML index of packages in '.Library'\n\nMaking 'packages.html' ...\n done\n\n\n\n\nlibrary(tidyverse)  # 데이터 처리 및 시각화\nlibrary(caret)      # 데이터 전처리, 모델링\nlibrary(FNN)        # KNN 회귀 함수 사용\n\n\n\n2. Data\n\n# 데이터 불러오기 (insurance.csv 파일 경로는 사용자 환경에 맞게 설정)\ndt &lt;- read.csv(\"insurance.csv\")\n\n# 기술통계\nsummary(dt)\n\n      age            sex                 bmi           children    \n Min.   :18.00   Length:1338        Min.   :15.96   Min.   :0.000  \n 1st Qu.:27.00   Class :character   1st Qu.:26.30   1st Qu.:0.000  \n Median :39.00   Mode  :character   Median :30.40   Median :1.000  \n Mean   :39.21                      Mean   :30.66   Mean   :1.095  \n 3rd Qu.:51.00                      3rd Qu.:34.69   3rd Qu.:2.000  \n Max.   :64.00                      Max.   :53.13   Max.   :5.000  \n    smoker             region             charges     \n Length:1338        Length:1338        Min.   : 1122  \n Class :character   Class :character   1st Qu.: 4740  \n Mode  :character   Mode  :character   Median : 9382  \n                                       Mean   :13270  \n                                       3rd Qu.:16640  \n                                       Max.   :63770  \n\n\n\n\n3. 전처리\n- 범주형 변수 처리 및 범주형으로 변환\n\ndt &lt;- dt %&gt;%\n  mutate(\n    sex = as.factor(sex),\n    smoker = as.factor(smoker),\n    region = as.factor(region)\n  ) %&gt;%\n  mutate_if(is.factor, as.character) %&gt;%   \n  mutate_if(is.character, as.factor)       \n\n- One-hot Encoding(fullRank=True로 다중공선성 방지)\n\ndmy &lt;- dummyVars(\" ~ .\", data = dt, fullRank = TRUE)\ndt_transformed &lt;- data.frame(predict(dmy, newdata = dt))\n\n- 독립변수(X), 종속변수(y) 분리\n\nX &lt;- dt_transformed[, -which(names(dt_transformed) == \"charges\")]\ny &lt;- dt_transformed$charges\n\n- 독립변수 표준화(평균0, 표준편차1)\n\n\n4. 적합\n\npre_proc &lt;- preProcess(X, method = c(\"center\", \"scale\"))\nX_scaled &lt;- predict(pre_proc, X)\n\n- 반복횟수 및 훈련 데이터 비율 설정\n\nT &lt;- 100                        # 100번 반복\nN &lt;- nrow(X_scaled)            # 전체 데이터 수\nn_train &lt;- round(0.7 * N)      # 70% 훈련 데이터\n\n- 다양한 k값(1~100)에 대한 RMSE 저장 행렬 생성\n\nk_range &lt;- 1:100\nrmse_matrix &lt;- matrix(NA, nrow = T, ncol = length(k_range))  # (T x 100) 행렬\n\n- 100번 반복하여 훈련/테스트 분할 및 RMSE 계산\n\nfor (t in 1:T) {\n  set.seed(t)  # 반복마다 시드 고정\n  train_idx &lt;- sample(1:N, n_train, replace = FALSE)\n  test_idx &lt;- setdiff(1:N, train_idx)\n  \n  x_train &lt;- X_scaled[train_idx, ]\n  x_test &lt;- X_scaled[test_idx, ]\n  y_train &lt;- y[train_idx]\n  y_test &lt;- y[test_idx]\n  \n  for (k in k_range) {\n    # KNN 회귀 실행\n    pred &lt;- knn.reg(\n      train = x_train,\n      test = x_test,\n      y = y_train,\n      k = k\n    )$pred\n    \n    # RMSE 계산 후 저장\n    rmse &lt;- sqrt(mean((y_test - pred)^2))\n    rmse_matrix[t, k] &lt;- rmse\n  }\n}\n\n- k별 평균 RMSE 계산\n\navg_rmse &lt;- colMeans(rmse_matrix, na.rm = TRUE)\nresults &lt;- data.frame(k = k_range, RMSE = avg_rmse)\n\n- RMSE 시각화\n\nggplot(results, aes(x = k, y = RMSE)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"black\") +\n  labs(title = \"K-NN Regression (100 different train/test splits)\",\n       x = \"k\",\n       y = \"Average RMSE\") +\n  theme_minimal()\n\nWarning message:\n“Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.”\n\n\n\n\n\n\n\n\n\n- 최적의 k 값 출력\n\nbest_k &lt;- results$k[which.min(results$RMSE)]\ncat(\"★ 최적의 k 값:\", best_k, \"\\n\")\n\n★ 최적의 k 값: 7 \n\n\n- 최종 예측 비교\n\nset.seed(999)\ntrain_index &lt;- sample(1:N, n_train, replace = FALSE)\ntest_index &lt;- setdiff(1:N, train_index)\n\n- 최적 k로 예측\n\nbest_pred &lt;- knn.reg(\n  train = X_scaled[train_index, ],\n  test = X_scaled[test_index, ],\n  y = y[train_index],\n  k = best_k\n)\n\n- k=100 으로 예측 (비교용)\n\nk100_pred &lt;- knn.reg(\n  train = X_scaled[train_index, ],\n  test = X_scaled[test_index, ],\n  y = y[train_index],\n  k = 100\n)\n\n- 예측값과 실제값의 상관계수 계산\n\ncor_best &lt;- cor(y[test_index], best_pred$pred)\ncor_k100 &lt;- cor(y[test_index], k100_pred$pred)\n\n- 시각화\n\npar(mfrow = c(1, 2))  # 1행 2열 그래프 레이아웃 설정\n\nplot(y[test_index] ~ best_pred$pred,\n     main = paste(\"k =\", best_k),\n     xlab = \"Predicted Charges\",\n     ylab = \"Actual Charges\",\n     col = \"black\", pch = 16)\nabline(0, 1, col = rgb(1, 0, 0, alpha = 0.3), lwd = 2)\ntext(x = mean(range(best_pred$pred)),\n     y = max(y[test_index]) * 0.95,\n     labels = paste(\"Correlation =\", round(cor_best, 3)),\n     col = \"black\", cex = 1.2, font = 1)\nlegend(\"topleft\",\n       legend = \"y = x\",\n       col = rgb(1, 0, 0, 0.3),\n       lty = 1,\n       lwd = 2,\n       bty = \"n\")\n\nplot(y[test_index] ~ k100_pred$pred,\n     main = \"k = 100\",\n     xlab = \"Predicted Charges\",\n     ylab = \"Actual Charges\",\n     col = \"black\", pch = 16)\nabline(0, 1, col = rgb(1, 0, 0, alpha = 0.3), lwd = 2)\ntext(x = mean(range(k100_pred$pred)),\n     y = max(y[test_index]) * 0.95,\n     labels = paste(\"Correlation =\", round(cor_k100, 3)),\n     col = \"black\", cex = 1.2, font = 1)\nlegend(\"topleft\",\n       legend = \"y = x\",\n       col = rgb(1, 0, 0, 0.3),\n       lty = 1,\n       lwd = 2,\n       bty = \"n\")\n\n\n\n\n\n\n\n\n\n- 레이아웃 초기화\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "posts/11.K-NN_Classification.html",
    "href": "posts/11.K-NN_Classification.html",
    "title": "11. K-NN Classification",
    "section": "",
    "text": "1. library\n\nlibrary(tidyverse)  # 데이터 처리 및 시각화\nlibrary(caret)      # 데이터 전처리, 분할, 평가\nlibrary(class)      # KNN 분류 함수\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nWarning message:\n“Your system is mis-configured: ‘/var/db/timezone/localtime’ is not a symlink”\nWarning message:\n“‘/var/db/timezone/localtime’ is not identical to any known timezone file”\n── Attaching core tidyverse packages ───────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ─────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLoading required package: lattice\n\n\nAttaching package: ‘caret’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    lift\n\n\n\nAttaching package: ‘gridExtra’\n\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\n\n\n\n\n\n2. Data\n\ndt &lt;- read.csv(\"fake_bills.csv\", sep = \";\")\n\n\n\n3. 전처리\n- 결측값 제거\n\ndt &lt;- na.omit(dt)\n\n- 목표변수 범주형으로 변환\n\ndt$is_genuine &lt;- as.factor(dt$is_genuine)\n\n- X,y분리\n\nX &lt;- dt %&gt;% select(-is_genuine)\ny &lt;- dt$is_genuine\n\n- 표준화\n\npre_proc &lt;- preProcess(X, method = c(\"center\", \"scale\"))\nX_scaled &lt;- predict(pre_proc, X)\n\n\n\n4. 적합\n- 반복 설정\n\nT &lt;- 100\nN &lt;- nrow(X_scaled)\nn_train &lt;- round(0.7 * N)\n\nk_range &lt;- 1:100\nacc_matrix &lt;- matrix(NA, nrow = T, ncol = length(k_range))\n\n- 반복 : train/test 나눠서 k별 정확도 저장\n\nN &lt;- nrow(X_scaled)\n\n\nfor (t in 1:T) {\n  set.seed(t)\n  train_idx &lt;- sample(1:N, n_train, replace = FALSE)\n  test_idx &lt;- setdiff(1:N, train_idx)\n  \n  x_train &lt;- X_scaled[train_idx, ]\n  x_test &lt;- X_scaled[test_idx, ]\n  y_train &lt;- y[train_idx]\n  y_test &lt;- y[test_idx]\n  \n  for (k in k_range) {\n    pred &lt;- knn(train = x_train, test = x_test, cl = y_train, k = k)\n    acc_matrix[t, k] &lt;- mean(pred == y_test)\n  }\n}\n\n- 평균 정확도 계산\n\navg_acc &lt;- colMeans(acc_matrix, na.rm = TRUE)\nresults &lt;- data.frame(k = k_range, Accuracy = avg_acc)\n\n- 시각화\n\nggplot(results, aes(x = k, y = Accuracy)) +\n  geom_line(color = \"darkgreen\", size = 1) +\n  geom_point(color = \"black\") +\n  labs(title = \"K-NN Classification (100 different train/test splits)\",\n       x = \"k\",\n       y = \"Average Accuracy\") +\n  theme_minimal()\n\nWarning message:\n“Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.”\n\n\n\n\n\n\n\n\n\n- 최적의 k\n\nbest_k &lt;- results$k[which.max(results$Accuracy)]\ncat(\"★ 최적의 k 값:\", best_k, \"\\n\")\n\n★ 최적의 k 값: 10 \n\n\n- 최종 예측 비교\n\nset.seed(999)\ntrain_index &lt;- sample(1:N, n_train, replace = FALSE)\ntest_index &lt;- setdiff(1:N, train_index)\n\nx_train &lt;- X_scaled[train_index, ]\nx_test &lt;- X_scaled[test_index, ]\ny_train &lt;- y[train_index]\ny_test &lt;- y[test_index]\n\n- 최적 k 예측\n\nbest_pred &lt;- knn(train = x_train, test = x_test, cl = y_train, k = best_k)\n\n- k=100 예측\n\nk100_pred &lt;- knn(train = x_train, test = x_test, cl = y_train, k = 100)\n\n- 정확도 비교\n\nacc_best &lt;- mean(best_pred == y_test)\nacc_k100 &lt;- mean(k100_pred == y_test)\n\ncat(\"✔️ Best k (\", best_k, \") 정확도:\", round(acc_best * 100, 2), \"%\\n\")\ncat(\"✔️ k = 100 정확도:\", round(acc_k100 * 100, 2), \"%\\n\")\n\n✔️ Best k ( 10 ) 정확도: 99.32 %\n✔️ k = 100 정확도: 98.63 %\n\n\n- 혼동행렬 출력\n\ncat(\"\\n▶ 혼동행렬 (Best k):\\n\")\nprint(confusionMatrix(best_pred, y_test))\n\ncat(\"\\n▶ 혼동행렬 (k = 100):\\n\")\nprint(confusionMatrix(k100_pred, y_test))\n\n\n▶ 혼동행렬 (Best k):\nConfusion Matrix and Statistics\n\n          Reference\nPrediction False True\n     False   153    0\n     True      3  283\n                                          \n               Accuracy : 0.9932          \n                 95% CI : (0.9802, 0.9986)\n    No Information Rate : 0.6446          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.985           \n                                          \n Mcnemar's Test P-Value : 0.2482          \n                                          \n            Sensitivity : 0.9808          \n            Specificity : 1.0000          \n         Pos Pred Value : 1.0000          \n         Neg Pred Value : 0.9895          \n             Prevalence : 0.3554          \n         Detection Rate : 0.3485          \n   Detection Prevalence : 0.3485          \n      Balanced Accuracy : 0.9904          \n                                          \n       'Positive' Class : False           \n                                          \n\n▶ 혼동행렬 (k = 100):\nConfusion Matrix and Statistics\n\n          Reference\nPrediction False True\n     False   150    0\n     True      6  283\n                                         \n               Accuracy : 0.9863         \n                 95% CI : (0.9705, 0.995)\n    No Information Rate : 0.6446         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.9699         \n                                         \n Mcnemar's Test P-Value : 0.04123        \n                                         \n            Sensitivity : 0.9615         \n            Specificity : 1.0000         \n         Pos Pred Value : 1.0000         \n         Neg Pred Value : 0.9792         \n             Prevalence : 0.3554         \n         Detection Rate : 0.3417         \n   Detection Prevalence : 0.3417         \n      Balanced Accuracy : 0.9808         \n                                         \n       'Positive' Class : False          \n                                         \n\n\n- 그래프로 출력\n\n# 혼동행렬 생성\ncm_best &lt;- confusionMatrix(best_pred, y_test)\ncm_k100 &lt;- confusionMatrix(k100_pred, y_test)\n\n# 데이터프레임 변환\ncm_best_df &lt;- as.data.frame(cm_best$table)\ncm_k100_df &lt;- as.data.frame(cm_k100$table)\n\n# 열의 순서 (True → False)\ncm_best_df$Reference  &lt;- factor(cm_best_df$Reference,  levels = c(\"True\", \"False\"))  \ncm_k100_df$Reference  &lt;- factor(cm_k100_df$Reference,  levels = c(\"True\", \"False\"))  \n\n- 그래프 1 : 최적k 혼동행렬\n\np1 &lt;- ggplot(cm_best_df, aes(x = Reference, y = Prediction)) +\n  geom_tile(fill = \"white\", color = \"black\", linewidth = 0.8) +  \n  geom_text(aes(label = Freq), size = 6, fontface = \"bold\") +\n  labs(title = paste0(\"Confusion Matrix (k = \", best_k, \")\"),\n       x = \"Actual\", y = \"Predicted\") +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    axis.text = element_text(size = 11, face = \"bold\"),\n    panel.grid = element_blank(),\n    legend.position = \"none\"\n  )\n\n- 그래프 2 : k=100 혼동행렬\n\np2 &lt;- ggplot(cm_k100_df, aes(x = Reference, y = Prediction)) +\n  geom_tile(fill = \"white\", color = \"black\", linewidth = 0.8) +\n  geom_text(aes(label = Freq), size = 6, fontface = \"bold\") +\n  labs(title = \"Confusion Matrix (k = 100)\",\n       x = \"Actual\", y = \"Predicted\") +\n  theme_minimal(base_family = \"Helvetica\") +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    axis.text = element_text(size = 11, face = \"bold\"),\n    panel.grid = element_blank(),\n    legend.position = \"none\"\n  )\n\n\ngrid.arrange(p1, p2, nrow = 1)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DC2025",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 4, 2025\n\n\n13. Graphics Python\n\n\n이상민 \n\n\n\n\nJun 2, 2025\n\n\n12. GR\n\n\n이상민 \n\n\n\n\nMay 28, 2025\n\n\n11. K-NN Classification\n\n\n이상민 \n\n\n\n\nMay 27, 2025\n\n\n10. K-NN Regression\n\n\n이상민 \n\n\n\n\nMay 19, 2025\n\n\n9. Regression\n\n\n이상민 \n\n\n\n\nMay 11, 2025\n\n\n8. Classifier\n\n\n이상민 \n\n\n\n\nApr 16, 2025\n\n\n7. Prediction Simulations 2\n\n\n이상민 \n\n\n\n\nApr 15, 2025\n\n\n6. Prediction Simulations 1\n\n\n이상민 \n\n\n\n\nApr 14, 2025\n\n\n5. Simulations ex\n\n\n이상민 \n\n\n\n\nApr 7, 2025\n\n\n4. 대수의 법칙, t-분포 ex\n\n\n이상민 \n\n\n\n\nApr 5, 2025\n\n\n3. DC HW1\n\n\n이상민 \n\n\n\n\nApr 4, 2025\n\n\n2. 상관계수 ex\n\n\n이상민 \n\n\n\n\nMar 25, 2025\n\n\n1. R distributions\n\n\n이상민 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/03.DC_HW1.html",
    "href": "posts/03.DC_HW1.html",
    "title": "3. DC HW1",
    "section": "",
    "text": "- 표본추출, 히스토그램\n\nmean = 3 으로 지정\n\n\nnormal_random &lt;- rnorm(n = 500, mean = 3)\nhist(normal_random)\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(normal_random)\n\n3.0025640596709\n\n\n\n\n\n- 표본추출, 히스토그램\n\n포아송 분포에서 \\(E(X) = \\lambda\\) 이므로 \\(\\lambda\\) = 3으로 지정\n\n\npoisson_random &lt;- rpois(n = 500, lambda = 3)\nhist(poisson_random)\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(poisson_random)\n\n2.874\n\n\n\n\n\n- 표본추출, 히스토그램\n\n이항분포에서 \\(E(X) = np =\\) 3이 되어야하고, size : n, prob = p 에 대응되므로 평균을 10 X 0.3 = 3으로 설정\n\n\nbinomial_random &lt;- rbinom(n = 500, size = 10, prob = 0.3)\nhist(binomial_random, breaks = seq(-0.5, 10.5, by = 1))\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(binomial_random)\n\n3.106\n\n\n\n\n\n- 표본추출, 히스토그램\n\n\\(GAM(\\alpha,\\beta)\\) 에서 shape : \\(\\alpha\\), rate : \\(\\frac{1}{\\beta}\\) 에 대응\n감마분포에서 \\(E(X) =\\) \\(\\alpha\\) X \\(\\beta =\\) 3이 되어야함\n1(shape) X 3(=1/rate) = 3\n\n\ngamma_random = rgamma(n=500,shape=1,rate=1/3)\nhist(gamma_random)\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(gamma_random)\n\n2.86597826745274\n\n\n\n\n\n- 표본추출, 히스토그램\n\n지수분포에서 \\(E(X)= \\frac{1}{\\lambda}=\\) 3이 되어야함\nrate= \\(\\lambda\\) 이므로 3(=1/rate)=3\n\n\nexponential_random &lt;- rexp(n = 500, rate = 1/3)\nhist(exponential_random)\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(exponential_random)\n\n3.00293798354794\n\n\n\n\n\n\n\n\n- 표본추출, 히스토그램\n\n이항분포에서 \\(E(X) = np\\) 이고\n이항분포에서 \\(Var(X) = np(1-p)\\) 임\nnp=3이어야하고 np(1-p)=5여야하므로 (1-p)=5/3 \\(\\to\\) (1-p)&gt;1 \\(\\to\\) 0&gt;p인 상황이므로 불가능하다. p는 확률이므로 &gt;=0\n이항분포에서 평균이 3, 분산이 5인 표본을 추출할 수 없다.\n\n\n\n\n- 표본추출, 히스토그램\n\nmean = 3 으로 지정\n\\(Var(X)=5\\) 이여야 하므로 sd=\\(\\sqrt{Var}\\)=\\(\\sqrt5\\) 로 지정\n\n\nnormal_random &lt;- rnorm(n = 500, mean = 3, sd=sqrt(5))\nhist(normal_random)\n\n\n\n\n\n\n\n\n- 표본평균, 표본분산\n\nmean(normal_random)\nvar(normal_random)\n\n2.92370604471217\n\n\n5.19843216215837\n\n\n\n\n\n- 표본추출, 히스토그램\n\n\\(GAM(\\alpha,\\beta)\\) 에서 shape : \\(\\alpha\\), rate : \\(\\frac{1}{\\beta}\\) 에 대응\n감마분포에서 \\(E(X) =\\) \\(\\alpha\\) X \\(\\beta\\) = 3이 되어야함\n\\(Var(X) =\\) \\(\\alpha\\) X \\(\\beta^2\\) = 5가 되어야함\n\\(\\to\\) \\(\\beta\\) = 5/3, \\(\\alpha\\) = 9/5 \\(\\to\\) rate = 3/5\n\n\ngamma_random = rgamma(n=500,shape=9/5,rate=3/5)\nhist(gamma_random)\n\n\n\n\n\n\n\n\n- 표본평균, 표본 분산\n\nmean(gamma_random)\nvar(gamma_random)\n\n3.15297204789198\n\n\n4.84604724922782"
  },
  {
    "objectID": "posts/03.DC_HW1.html#제공된-코드를-이용하여-아래를-시행하시오.",
    "href": "posts/03.DC_HW1.html#제공된-코드를-이용하여-아래를-시행하시오.",
    "title": "3. DC HW1",
    "section": "",
    "text": "- 표본추출, 히스토그램\n\nmean = 3 으로 지정\n\n\nnormal_random &lt;- rnorm(n = 500, mean = 3)\nhist(normal_random)\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(normal_random)\n\n3.0025640596709\n\n\n\n\n\n- 표본추출, 히스토그램\n\n포아송 분포에서 \\(E(X) = \\lambda\\) 이므로 \\(\\lambda\\) = 3으로 지정\n\n\npoisson_random &lt;- rpois(n = 500, lambda = 3)\nhist(poisson_random)\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(poisson_random)\n\n2.874\n\n\n\n\n\n- 표본추출, 히스토그램\n\n이항분포에서 \\(E(X) = np =\\) 3이 되어야하고, size : n, prob = p 에 대응되므로 평균을 10 X 0.3 = 3으로 설정\n\n\nbinomial_random &lt;- rbinom(n = 500, size = 10, prob = 0.3)\nhist(binomial_random, breaks = seq(-0.5, 10.5, by = 1))\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(binomial_random)\n\n3.106\n\n\n\n\n\n- 표본추출, 히스토그램\n\n\\(GAM(\\alpha,\\beta)\\) 에서 shape : \\(\\alpha\\), rate : \\(\\frac{1}{\\beta}\\) 에 대응\n감마분포에서 \\(E(X) =\\) \\(\\alpha\\) X \\(\\beta =\\) 3이 되어야함\n1(shape) X 3(=1/rate) = 3\n\n\ngamma_random = rgamma(n=500,shape=1,rate=1/3)\nhist(gamma_random)\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(gamma_random)\n\n2.86597826745274\n\n\n\n\n\n- 표본추출, 히스토그램\n\n지수분포에서 \\(E(X)= \\frac{1}{\\lambda}=\\) 3이 되어야함\nrate= \\(\\lambda\\) 이므로 3(=1/rate)=3\n\n\nexponential_random &lt;- rexp(n = 500, rate = 1/3)\nhist(exponential_random)\n\n\n\n\n\n\n\n\n- 표본평균\n\nmean(exponential_random)\n\n3.00293798354794\n\n\n\n\n\n\n\n\n- 표본추출, 히스토그램\n\n이항분포에서 \\(E(X) = np\\) 이고\n이항분포에서 \\(Var(X) = np(1-p)\\) 임\nnp=3이어야하고 np(1-p)=5여야하므로 (1-p)=5/3 \\(\\to\\) (1-p)&gt;1 \\(\\to\\) 0&gt;p인 상황이므로 불가능하다. p는 확률이므로 &gt;=0\n이항분포에서 평균이 3, 분산이 5인 표본을 추출할 수 없다.\n\n\n\n\n- 표본추출, 히스토그램\n\nmean = 3 으로 지정\n\\(Var(X)=5\\) 이여야 하므로 sd=\\(\\sqrt{Var}\\)=\\(\\sqrt5\\) 로 지정\n\n\nnormal_random &lt;- rnorm(n = 500, mean = 3, sd=sqrt(5))\nhist(normal_random)\n\n\n\n\n\n\n\n\n- 표본평균, 표본분산\n\nmean(normal_random)\nvar(normal_random)\n\n2.92370604471217\n\n\n5.19843216215837\n\n\n\n\n\n- 표본추출, 히스토그램\n\n\\(GAM(\\alpha,\\beta)\\) 에서 shape : \\(\\alpha\\), rate : \\(\\frac{1}{\\beta}\\) 에 대응\n감마분포에서 \\(E(X) =\\) \\(\\alpha\\) X \\(\\beta\\) = 3이 되어야함\n\\(Var(X) =\\) \\(\\alpha\\) X \\(\\beta^2\\) = 5가 되어야함\n\\(\\to\\) \\(\\beta\\) = 5/3, \\(\\alpha\\) = 9/5 \\(\\to\\) rate = 3/5\n\n\ngamma_random = rgamma(n=500,shape=9/5,rate=3/5)\nhist(gamma_random)\n\n\n\n\n\n\n\n\n- 표본평균, 표본 분산\n\nmean(gamma_random)\nvar(gamma_random)\n\n3.15297204789198\n\n\n4.84604724922782"
  },
  {
    "objectID": "posts/03.DC_HW1.html#아래와-같이-표본이-주어져-있다고-한다.",
    "href": "posts/03.DC_HW1.html#아래와-같이-표본이-주어져-있다고-한다.",
    "title": "3. DC HW1",
    "section": "3. 아래와 같이 표본이 주어져 있다고 한다.",
    "text": "3. 아래와 같이 표본이 주어져 있다고 한다.\n\n1) 두 변수 간의 표본과 공분산 그리고 표본 상관계수를 계산하고 해석하시오\n\nx = c(10,20,10,1,3,5,8,9,10)\ny = c(3,5,7,3,2,3,2,2,2)\n\n\ncov(x,y)\ncor(x,y)\n\n4.13888888888889\n\n\n0.442013916579573\n\n\n- 표본 공분산 = 4.138\n\n표본 공분산 : \\(S_{xy} &gt; 0\\) 이므로 x,y는 양의 선형적 관계를 가진다.\n\\(\\to\\) x가 증가할수록 y도 증가하는 경향을 보인다.\n\n- 표본 상관계수 = 0.442\n\n표본 상관계수[-1,1] : \\(r_{xy}\\) &gt; 0 이므로 양의 선형관계를 가진다.\n하지만 |\\(r_{xy}\\)|가 0.4정도로 약한 관계를 보인다.\n\\(\\to\\) x가 증가할수록 y도 증가하는 경향을 보이지만 그 관계는 약하다.\n\n\n\n2) (추가) \\(x\\) \\(\\to\\) \\(2x+1\\), \\(y\\) \\(\\to\\) \\(-y+2\\)로 변환하고 나서 표본 상관계수를 계산하시오.\n\ncor(2*x+1,-y+2)\n\n-0.442013916579573\n\n\n- 표본 상관계수 = -0.442\n\n상관계수의 크기는 선형 변환에 따라 바뀌지 않음\n만약 \\(aX+b, cX+d\\) 에서\n\n\n\\(a\\) X \\(b\\) \\(&lt; 0\\) 이면 \\(\\to\\) Corr(aX,cY) = -Corr(X,Y)\n\n\n\\(a\\) X \\(b\\) \\(&gt; 0\\) 이면 \\(\\to\\) Corr(aX,cY) = Corr(X,Y)\n\n\n여기서 2 X -1 = -2이므로 1)의 경우이므로 표본 상관계수의 부호가 변하였다."
  },
  {
    "objectID": "posts/12.GR.html",
    "href": "posts/12.GR.html",
    "title": "12. GR",
    "section": "",
    "text": "set.seed(11)\nrbinom(2500, 1, 0.5) -&gt; ipp \ndf &lt;- data.frame(\nsex=factor(rep(c(\"A\", \"B\"), each=2500)), weight=round(c(rnorm(2500, mean=55, sd=5),\nipp* rnorm(2500, mean=65, sd=5)                                                     + (1-ipp)*rnorm(2500, mean=45, sd=3) ) ))\nhead(df) ; tail(df)\n\n\nA data.frame: 6 × 2\n\n\n\nsex\nweight\n\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n\n\n\n\n1\nA\n56\n\n\n2\nA\n64\n\n\n3\nA\n57\n\n\n4\nA\n48\n\n\n5\nA\n55\n\n\n6\nA\n57\n\n\n\n\n\n\nA data.frame: 6 × 2\n\n\n\nsex\nweight\n\n\n\n&lt;fct&gt;\n&lt;dbl&gt;\n\n\n\n\n4995\nB\n65\n\n\n4996\nB\n45\n\n\n4997\nB\n68\n\n\n4998\nB\n66\n\n\n4999\nB\n47\n\n\n5000\nB\n63\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(df, aes(x=weight)) + geom_histogram()\nlibrary(ggplot2)\nggplot(df, aes(x=weight)) + geom_histogram() + geom_histogram(color='blue', fill='orange')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(df, aes(x=weight)) + geom_histogram(binwidth=10, color=\"blue\", fill=\"orange\")\nlibrary(ggplot2)\nggplot(df, aes(x=weight)) + geom_histogram(binwidth=0.1, color=\"blue\", fill=\"orange\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(df, aes(x=weight)) + geom_histogram(aes(y=..density..), \n  colour=\"black\", fill=\"white\")+ geom_density(alpha=.2, fill=\"#FF6666\") + \n  # geom_histogram(position=\"dodge\")+ \n  theme(legend.position=\"top\") + theme(axis.title = element_text(size=20),\n  legend.title = element_text(size=20), \n  legend.text = element_text(size=20), axis.text = element_text(size=20))\n\nWarning message:\n“The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.”\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(df, aes(x=weight, color=sex, fill=sex)) + \n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+ \n  geom_density(alpha= 0.2, fill=\"#FF6666\") +\n  theme(legend.position=\"top\") + \n  theme(axis.title = element_text(size=20), legend.title = element_text(size=20),\n  legend.text = element_text(size=20), axis.text = element_text(size=20))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nhead(OrchardSprays)\nggplot(OrchardSprays, aes(x=factor(treatment), y=decrease, fill=factor(treatment)))+\n  geom_violin(adjust=0.5, scale='width', color = rainbow(1))+\n  geom_boxplot(size=1, width=0.5)+ theme_minimal()\n\n\nA data.frame: 6 × 4\n\n\n\ndecrease\nrowpos\ncolpos\ntreatment\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n1\n57\n1\n1\nD\n\n\n2\n95\n2\n1\nE\n\n\n3\n8\n3\n1\nB\n\n\n4\n69\n4\n1\nH\n\n\n5\n92\n5\n1\nG\n\n\n6\n90\n6\n1\nF\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata(VADeaths) \n# par(mfrow=c(1,1))\ncolnames(VADeaths) = c('rM','rF','uM','uF') \nbarplot(round(VADeaths[1,]), col=2:5, border='white', ylim=c(0,25), main= 'VADeaths (50-54)')\n\n\n\n\n\n\n\n\n\nrequire(ggplot2) ; data(UCBAdmissions) ; \nstr(UCBAdmissions) \naddmargins(apply(UCBAdmissions, c(2,1), sum))\nrequire(graphics) \nmosaicplot(apply(UCBAdmissions, c(2,1), sum),\n           color=c('gray','red'), main ='UC Berkeley Admissions' )\n\n 'table' num [1:2, 1:2, 1:6] 512 313 89 19 353 207 17 8 120 205 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ Admit : chr [1:2] \"Admitted\" \"Rejected\"\n  ..$ Gender: chr [1:2] \"Male\" \"Female\"\n  ..$ Dept  : chr [1:6] \"A\" \"B\" \"C\" \"D\" ...\n\n\n\nA matrix: 3 × 3 of type dbl\n\n\n\nAdmitted\nRejected\nSum\n\n\n\n\nMale\n1198\n1493\n2691\n\n\nFemale\n557\n1278\n1835\n\n\nSum\n1755\n2771\n4526\n\n\n\n\n\n\n\n\n\n\n\n\n\nrequire(graphics) \nmosaicplot(apply(UCBAdmissions, c(2,1), sum), \n  color=c('gray','red'), main ='UC Berkeley Admissions' )\nmosaicplot(~Dept+Gender+Admit, data=UCBAdmissions, \n           color=c('gray','red'), dir = c('v','v','h'), \n           main ='UC Berkeley Admissions' )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(treemap) \ndata(GNI2014) ; str(GNI2014)\ntreemap(GNI2014, index = c('continent','iso3'), \n        vSize='population', vColor='GNI', type='value')\n\n'data.frame':   188 obs. of  5 variables:\n $ iso3      : chr  \"BMU\" \"NOR\" \"QAT\" \"CHE\" ...\n $ country   : chr  \"Bermuda\" \"Norway\" \"Qatar\" \"Switzerland\" ...\n $ continent : Factor w/ 8 levels \"Africa\",\"Antarctica\",..: 5 4 3 4 3 4 6 4 4 5 ...\n $ population: num  67837 4676305 833285 7604467 559846 ...\n $ GNI       : int  106140 103630 92200 88120 76270 75990 64540 61610 61310 55200 ...\n\n\n\n\n\n\n\n\n\n\nGNI2014$GNI.total = GNI2014$GNI * GNI2014$population\nGNI2014.a = aggregate(GNI2014[,4:6], by = list(GNI2014$continent), sum) \nGNI2014.a$GNI = GNI2014.a$GNI.total/GNI2014.a$population \ntreemap(GNI2014.a, index = c('Group.1'), vSize='population', \n        vColor='GNI', type='value' )"
  },
  {
    "objectID": "posts/05.Simulations_ex.html",
    "href": "posts/05.Simulations_ex.html",
    "title": "5. Simulations ex",
    "section": "",
    "text": "Simulation for CLT\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skewnorm\n\ndef simulate_clt(distribution_type, sample_size, num_samples, **kwargs):\n    \"\"\"\n    중심극한정리(CLT) 시뮬레이션 함수\n\n    Args:\n        distribution_type (str): 모집단 분포 종류 ('uniform', 'exponential', 'normal', 'skewed_left', 'skewed_right')\n        sample_size (int): 각 표본의 크기\n        num_samples (int): 표본의 개수\n        **kwargs: 각 분포에 따른 추가 매개변수\n            - uniform: low (default=0), high (default=1)\n            - exponential: scale (default=1)  (scale = 1/lambda)\n            - normal: loc (default=0), scale (default=1)  (loc: 평균, scale: 표준편차)\n            - binomal: (n, p)\n            - gamma: (shape, scale)\n\n    Returns:\n        None (히스토그램 시각화)\n    \"\"\"\n    sample_means = []\n    for _ in range(num_samples):\n        if distribution_type == 'uniform':\n            sample = np.random.uniform(low=kwargs.get('low', 0), high=kwargs.get('high', 1), size=sample_size)\n        elif distribution_type == 'exponential':\n            sample = np.random.exponential(scale=kwargs.get('scale', 1), size=sample_size)\n        elif distribution_type == 'normal':\n            sample = np.random.normal(loc=kwargs.get('loc', 0), scale=kwargs.get('scale', 1), size=sample_size)\n        elif distribution_type == 'binomial':\n            sample = np.random.binomial(kwargs.get('n', 100), kwargs.get('p', 0.5), size=sample_size)\n        elif distribution_type == 'gamma':\n            sample = np.random.gamma(3, 2, size=sample_size)\n        else:\n            raise ValueError(\"Invalid distribution type.\")\n        sample_means.append(np.mean(sample))\n\n    plt.figure(figsize=(8, 6))\n    sns.histplot(sample_means, kde=True, stat=\"density\")\n    plt.title(f\"Distribution of Sample Means (n={sample_size}, {num_samples} samples)\\nFrom {distribution_type} distribution\", fontsize=14)\n    plt.xlabel(\"Sample Mean\", fontsize=12)\n    plt.ylabel(\"Density\", fontsize=12)\n    plt.show()\n    return sample_means\n\n\n# 실험 설정\ndistributions = {\n#     'uniform': {'low': 0, 'high': 1},\n#     'exponential': {'scale': 1},\n#     'normal': {'loc': 0, 'scale': 1},\n#      'binomial': {'n': 100, 'p': 0.1},\n     'gamma': {'shape': 3, 'scale': 2},\n}\n\nsample_sizes = [5, 100, 1000]\nnum_samples_list = [1000]\n\n\n#  다양한 분포, 표본 크기, 표본 개수에 대한 실험\nfor dist_name, dist_params in distributions.items():\n    for sample_size in sample_sizes:\n        for num_samples in num_samples_list:\n            print(f\"\\n--- {dist_name}, sample_size={sample_size}, num_samples={num_samples} ---\")\n            simulate_clt(dist_name, sample_size, num_samples, **dist_params)\n\n\n--- gamma, sample_size=5, num_samples=1000 ---\n\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\n--- gamma, sample_size=100, num_samples=1000 ---\n\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\n--- gamma, sample_size=1000, num_samples=1000 ---\n\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n- 정규분포라고 하기에는 약간 애매\n\n분포를 바꿔가면서 할 수 있음!!!\n\n\n\n2. Simulation for Confidence Interval\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, t  # 정규분포, t-분포 사용\n\n\ndef confidence_interval_simulation(population_mean, population_std, sample_size, num_samples, confidence_level=0.95, distribution='normal'):\n    \"\"\"\n    신뢰구간 시뮬레이션 함수\n\n    Args:\n        population_mean (float): 모집단 평균\n        population_std (float): 모집단 표준편차\n        sample_size (int): 표본 크기\n        num_samples (int): 표본 추출 횟수 (시뮬레이션 반복 횟수)\n        confidence_level (float): 신뢰 수준 (default: 0.95)\n        distribution (str): 'normal' (정규분포) 또는 't' (t-분포) (default: 'normal')\n\n    Returns:\n        None (신뢰구간 시각화 및 포함 비율 출력)\n    \"\"\"\n\n    if distribution not in ['normal', 't']:\n        raise ValueError(\"distribution must be 'normal' or 't'\")\n\n    intervals_containing_mean = 0  # 모집단 평균을 포함하는 신뢰구간 개수\n    plt.figure(figsize=(10, 6))\n\n    for _ in range(num_samples):\n        # 1. 표본 추출\n        sample = np.random.normal(loc=population_mean, scale=population_std, size=sample_size)\n        sample_mean = np.mean(sample)\n        sample_std = np.std(sample, ddof=1)  # 표본 표준편차 (불편추정량, ddof=1)\n\n        # 2. 신뢰구간 계산\n        if distribution == 'normal':\n            # 정규분포 기반 (모집단 표준편차를 알 때)\n            z_critical = norm.ppf((1 + confidence_level) / 2)  # Z-critical value\n            margin_of_error = z_critical * (population_std / np.sqrt(sample_size))\n            lower_bound = sample_mean - margin_of_error\n            upper_bound = sample_mean + margin_of_error\n        else:  # distribution == 't'\n            # t-분포 기반 (모집단 표준편차를 모를 때)\n            t_critical = t.ppf((1 + confidence_level) / 2, df=sample_size - 1) # t-critical value\n            margin_of_error = t_critical * (sample_std / np.sqrt(sample_size))  # 표본표준편차 사용\n            lower_bound = sample_mean - margin_of_error\n            upper_bound = sample_mean + margin_of_error\n\n        # 3. 신뢰구간이 모집단 평균을 포함하는지 확인\n        if lower_bound &lt;= population_mean &lt;= upper_bound:\n            intervals_containing_mean += 1\n            color = 'green'  # 포함하면 파란색\n        else:\n            color = 'blue'  # 포함하지 않으면 빨간색\n\n        # 4. 신뢰구간 시각화\n        plt.plot([lower_bound, upper_bound], [_, _], color=color, alpha=0.7)\n        plt.scatter(sample_mean, _, color=color, s=10, alpha=0.7)  # 표본평균 표시\n\n    # 5. 결과 출력\n    plt.axvline(x=population_mean, color='green', linestyle='--', label=f'Population Mean ({population_mean})')  # 모집단 평균 (녹색 점선)\n    plt.title(f\"Confidence Interval Simulation ({confidence_level*100:.0f}% CI)\\n\"\n              f\"Sample Size: {sample_size},  Number of Samples: {num_samples}, Distribution: {distribution}\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Sample Number\")\n    plt.legend()\n    plt.show()\n\n    inclusion_rate = (intervals_containing_mean / num_samples) * 100\n    print(f\"Inclusion Rate: {inclusion_rate:.2f}%  ({intervals_containing_mean} out of {num_samples} intervals contained the population mean)\")\n\n# 시뮬레이션 실행 예시\npopulation_mean = 100  # 모집단 평균\npopulation_std = 15    # 모집단 표준편차\nsample_size = 100       # 표본 크기\nnum_samples = 100      # 표본 추출 횟수\n\n# 정규분포 기반 신뢰구간 시뮬레이션\nconfidence_interval_simulation(population_mean, population_std, sample_size, num_samples, confidence_level=0.95, distribution='normal')\n# t-분포 기반 신뢰구간 시뮬레이션 (모표준편차 모를 때)\n#confidence_interval_simulation(population_mean, population_std, sample_size, num_samples, confidence_level=0.95, distribution='t')\n\n\n\n\n\n\n\n\nInclusion Rate: 92.00%  (92 out of 100 intervals contained the population mean)\n\n\n- 코드는 몰라도 됨\n\n결과에 대한 분석만 할 줄 알면 됨\nconfidence_level 바꿔가며 실험\n표본크기 바꿔가며 실험 \\(\\to\\) 표본 크기에 따라 양상이 달라짐\n\n\n\n3. Example of indep. two sample t.test\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport numpy as np\n\n# 1. 데이터 불러오기 및 확인\n\n# CSV 파일 읽기\ndf = pd.read_csv(\"GG.csv\")\nprint(\"원본 데이터:\\n\", df)\n\n# 2. 데이터 분리 및 결측치 처리\n\n# male, female 데이터를 각각 NumPy 배열로 변환\nmale_height = df['male'].to_numpy()\nfemale_height = df['female'].to_numpy()\n\n# NumPy의 isnan() 함수를 사용하여 NaN 값 제거\nmale_height_cleaned = male_height[~np.isnan(male_height)].astype(float)\nfemale_height_cleaned = female_height[~np.isnan(female_height)].astype(float)\n\nprint(\"\\n결측치 제거 후 male 키 데이터:\\n\", male_height_cleaned)\nprint(\"\\n결측치 제거 후 female 키 데이터:\\n\", female_height_cleaned)\n\n# 3. 독립표본 t-검정 (등분산 가정)\n# 귀무가설 (H0): 남학생 키의 모평균과 여학생 키의 모평균은 같다. (모분산 같음)\n# 대립가설 (H1): 남학생 키의 모평균과 여학생 키의 모평균은 다르다.\n\n# equal_var=True (등분산 가정)\nt_stat_equal, p_val_equal = ttest_ind(male_height_cleaned, female_height_cleaned, equal_var=True)\n\nprint(\"\\n--- 등분산 가정 t-검정 ---\")\nprint(\"t-statistic:\", t_stat_equal)\nprint(\"p-value:\", p_val_equal)\n\n# 결과 해석 (등분산 가정)\nalpha = 0.05  # 유의 수준\nif p_val_equal &lt; alpha:\n    print(\"등분산 가정: 귀무가설 기각 - 남녀 키 평균은 통계적으로 유의미하게 다릅니다.\")\nelse:\n    print(\"등분산 가정: 귀무가설 채택 - 남녀 키 평균은 통계적으로 유의미한 차이가 없습니다.\")\n\n# 4. 독립표본 t-검정 (이분산 가정)\n# 귀무가설 (H0): 남학생 키의 모평균과 여학생 키의 모평균은 같다. (모분산 다름)\n# 대립가설 (H1): 남학생 키의 모평균과 여학생 키의 모평균은 다르다.\n\n# equal_var=False (이분산 가정, Welch's t-test)\nt_stat_welch, p_val_welch = ttest_ind(male_height_cleaned, female_height_cleaned, equal_var=False)\n\nprint(\"\\n--- 이분산 가정 t-검정 (Welch's t-test) ---\")\nprint(\"t-statistic:\", t_stat_welch)\nprint(\"p-value:\", p_val_welch)\n\n# 결과 해석 (이분산 가정)\nif p_val_welch &lt; alpha:\n    print(\"이분산 가정: 귀무가설 기각 - 남녀 키 평균은 통계적으로 유의미하게 다릅니다.\")\nelse:\n    print(\"이분산 가정: 귀무가설 채택 - 남녀 키 평균은 통계적으로 유의미한 차이가 없습니다.\")\n\n#### 참고사항: scipy 버전에 따라 자유도가 출력될 수 있음: 각자의 컴퓨터마다 아래 결과에 \"df =\"출력될 수 있으니 돌려보기만 하면 됩니다.\n\nprint(ttest_ind(male_height_cleaned, female_height_cleaned, equal_var=True))\nprint(ttest_ind(male_height_cleaned, female_height_cleaned, equal_var=False))\n\n원본 데이터:\n     male  female\n0    114   108.0\n1     96    98.0\n2     80    88.0\n3    102    86.0\n4     94   100.0\n5     94    98.0\n6     98   104.0\n7     92   102.0\n8     94    94.0\n9    100     NaN\n10   108     NaN\n11   110     NaN\n12    90     NaN\n13    90     NaN\n14    82     NaN\n15   106     NaN\n\n결측치 제거 후 male 키 데이터:\n [114.  96.  80. 102.  94.  94.  98.  92.  94. 100. 108. 110.  90.  90.\n  82. 106.]\n\n결측치 제거 후 female 키 데이터:\n [108.  98.  88.  86. 100.  98. 104. 102.  94.]\n\n--- 등분산 가정 t-검정 ---\nt-statistic: -0.18597961132240054\np-value: 0.8540912649163084\n등분산 가정: 귀무가설 채택 - 남녀 키 평균은 통계적으로 유의미한 차이가 없습니다.\n\n--- 이분산 가정 t-검정 (Welch's t-test) ---\nt-statistic: -0.20139794503778663\np-value: 0.8423466136276512\n이분산 가정: 귀무가설 채택 - 남녀 키 평균은 통계적으로 유의미한 차이가 없습니다.\nTtestResult(statistic=-0.18597961132240054, pvalue=0.8540912649163084, df=23.0)\nTtestResult(statistic=-0.20139794503778663, pvalue=0.8423466136276512, df=20.771503751242072)\n\n\n- p-value가 너무 커서 귀무가설 기각하지 못함"
  },
  {
    "objectID": "posts/13.Graphics_Python.html",
    "href": "posts/13.Graphics_Python.html",
    "title": "13. Graphics Python",
    "section": "",
    "text": "1. imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n\n2. 시각화\n\nA. Histogrm\n- Histogrm (bin) : 구간의 넓이를 조정\n\nweight = [68, 81, 64, 56, 78, 74, 61, 77, 66, 68, 59, 71,\n          80, 59, 67, 81, 69, 73, 69, 74, 70, 65]\nplt.hist(weight)\nplt.show()\n\nplt.hist(weight, label='bins=10')\nplt.hist(weight, bins=30, label='bins=30')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplt.hist(weight, cumulative=True, label='cumulative=True')\nplt.hist(weight, cumulative=False, label='cumulative=False')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n- Histogrm (type) : 히스토그램의 유형\n\nweight2 = [52, 67, 84, 66, 58, 78, 71, 57, 76, 62, 51, 79,\n        69, 64, 76, 57, 63, 53, 79, 64, 50, 61]\n\nplt.hist((weight), histtype='bar')\nplt.title('histtype - bar')\nplt.figure()\n\nplt.hist((weight), histtype='barstacked')\nplt.title('histtype - barstacked')\nplt.figure()\n\nplt.hist((weight), histtype='step')\nplt.title('histtype - step')\nplt.figure()\n\nplt.hist((weight, weight2), histtype='stepfilled')\nplt.title('histtype - stepfilled')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- Histogrm (density) : 밀도함수\n\nimport seaborn as sns\nimport numpy as np\n\naa = np.random.normal(20, 1, 1000)\nbb = np.random.normal(25, 3, 500)\ncc = np.random.normal(10, 3, 500)\nweight = np.concatenate([aa,bb,cc])\n\nsns.displot(weight, kde=True,\n             bins=10, color = 'darkblue')\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\n\nB. Boxplot\n- Boxplot (Categorical) : 기본유형\n\ntips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n\n\n\n\n\n\n\n\n\n- Boxplot (Categorical) : 변동과 점 구분\n\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\", kind=\"swarm\")\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/_oldcore.py:1075: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  data_subset = grouped_data.get_group(pd_key)\n\n\n\n\n\n\n\n\n\n- boxplot (Categorical) : 구분자로 분포의 차이를 확인\n\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"box\")\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\n\n\n\n\n\n\n\n\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"smoker\", kind=\"box\")\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\n\n\n\n\n\n\n\n- Boxplot (바이올린) : 분포의 형태를 같이 봄\n\nsns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\", kind=\"violin\",\n)\nsns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\",\n    kind=\"violin\", bw_adjust=.5, cut=0,\n)\n\nsns.catplot(\n    data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\",\n    kind=\"violin\", split=True,\n)\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC. Barplot\n- Barplot : 단순유형\n\ntitanic = sns.load_dataset(\"titanic\")\nsns.catplot(data=titanic, x=\"deck\", kind=\"count\")\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\n\n\n\n\n\n\n\n- Barplot : 비교유형\n\nsns.catplot(data=titanic, x=\"sex\", y=\"survived\", hue=\"class\", kind=\"bar\")\nsns.catplot(data=titanic, x=\"age\", y=\"deck\", errorbar=(\"pi\", 95), kind=\"bar\")\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.mosaicplot import mosaic\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = [12, 8]\n\nimport pandas as pd\ndf = pd.read_csv(\"https://waf.cs.illinois.edu/discovery/berkeley.csv\")\ndf\ndf = df[['Major', 'Gender', 'Admission']]\n\nmosaic(df.sort_values('Gender'), ['Gender', 'Admission'],\n       title='Berkeley Admission')\nplt.show()\n\nmosaic(df.sort_values('Gender'),['Major', 'Admission','Gender'],\n       title='Berkeley Admission')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nD. Scatter Plot\n\nnp.random.seed(0)\nmean = [0, 0]\ncov = [[1, 0.8], [0.8, 1]]\nX = np.random.multivariate_normal(mean, cov, 100)\ndf = pd.DataFrame(X, columns=['X1', 'X2'])\n\n\nsns.scatterplot(x='X1', y='X2', data=df)\nplt.title('산점도 (Seaborn)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nmean_x = df['X1'].mean()\nmean_y = df['X2'].mean()\n\nplt.figure(figsize=(8, 6))\nplt.scatter(df['X1'], df['X2'])\nplt.plot([mean_x, mean_x], [min(df['X2']), max(df['X2'])], 'r--', label=f'평균 X1: {mean_x:.2f}')\nplt.plot([min(df['X1']), max(df['X1'])], [mean_y, mean_y], 'r--', label=f'평균 X2: {mean_y:.2f}')\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.title('산점도 및 평균 수준')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n- 구글 AI 지수 시계열 분석: 주요 파장의 값을 뽑아내고 상관관계를 파악\n\nts  = pd.read_csv('TS1.csv')\nts = ts.to_numpy()\nll = ts.shape[0]\nts = np.array(ts[range(2,ll),1], dtype='float')\nplt.plot(ts)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy import signal\nf, t, Zxx = signal.stft(ts, fs=int(ll-2), nperseg=100)\nprint(Zxx.shape)\nplt.plot(Zxx)\nplt.show()\n\nplt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=50)\nplt.show()\n\n(17, 3)\n\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 100 is greater than input length  = 32, using nperseg = 32\n  warnings.warn('nperseg = {0:d} is greater than input length '\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1369: ComplexWarning: Casting complex values to real discards the imaginary part\n  return np.asarray(x, float)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncovariance_matrix = pd.DataFrame(Zxx).corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(covariance_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Covariance Heatmap')\nplt.show()\n\n/root/anaconda3/envs/pypy/lib/python3.10/site-packages/pandas/core/internals/managers.py:1688: ComplexWarning: Casting complex values to real discards the imaginary part\n  arr = np.array(blk.values, dtype=dtype, copy=copy)"
  },
  {
    "objectID": "posts/04.대수의법칙,t분포ex.html",
    "href": "posts/04.대수의법칙,t분포ex.html",
    "title": "4. 대수의 법칙, t-분포 ex",
    "section": "",
    "text": "1. 대수의 법칙\n\ncumsum(rpois(5000,3))[1:20]\n\n\n34589131719232431343741444651555860\n\n\n\n(cumsum(rpois(5000,3))/1:5000)[1:20]\n\n\n111.33333333333333222.52.7142857142857133.111111111111113.13.090909090909093333.066666666666673.1253.176470588235293.111111111111113.210526315789473.25\n\n\n\nts.plot(cumsum(rpois(5000,3))/1:5000)\n\n\n\n\n\n\n\n\n- 모평균이 3이므로 3으로 가까이 수렴하는 형태\n\n분포 종류 상관 x\n대수의 법칙\n\n\nrs=rep(0,1000)\nfor(k in 1:1000)\n    {\n    rs[k]=mean(rpois(1000*k,3))\n\n    if(k%%100 == 0) print(k)\n    }\n\n[1] 100\n[1] 200\n[1] 300\n[1] 400\n[1] 500\n[1] 600\n[1] 700\n[1] 800\n[1] 900\n[1] 1000\n\n\n\nplot(rs)\n\n\n\n\n\n\n\n\n- 표본의 크기가 커지면 더 가늘어짐\n\nrs=rep(0,1000)\nfor(k in 1:1000)\n    {\n    rs[k]=mean(rpois(2000*k,3))\n\n    if(k%%100 == 0) print(k)\n    }\n\n[1] 100\n[1] 200\n[1] 300\n[1] 400\n[1] 500\n[1] 600\n[1] 700\n[1] 800\n[1] 900\n[1] 1000\n\n\n\nplot(rs,ylim = c(2.94, 3.04))\n\n\n\n\n\n\n\n\n\n\n2. t-분포\n- t-분포 자유도에 따른 차이 시각화\n\nred : 자유도 5\nblue : 자유도 100\n자유도가 무한이면 표준정규분포로 수렴\n\n\nplot(dt(seq(-5,5,length=100),5)~seq(-5,5,length=100), type='l', col='red')\npoints(dt(seq(-5,5,length=100),100)~seq(-5,5,length=100), type='l', col='blue')"
  },
  {
    "objectID": "posts/02.상관계수ex.html",
    "href": "posts/02.상관계수ex.html",
    "title": "2. 상관계수 ex",
    "section": "",
    "text": "- 상관계수\n\nX,Y에 상수를 곱해줘도 그대로, 변하지 않음\n기울기만 변함\n\n\nx = rnorm(100)\ny = rnorm(100) + 2*x\nplot(x,y)\ncor(x,y)\npoints(2*x, 0.25*y, col='red') \ncor(2*x, 0.25*y)\n\n0.881075342687499\n\n\n0.881075342687499\n\n\n\n\n\n\n\n\n\n- 상관계수\n\naX, bY 의 상관계수에서 aXb &lt;0 이면\nCorr(aX,bY) = -Corr(X,Y)\n상관계수 크기는 그대로\n기울기만 변함\n\n\nx = rnorm(100)\ny = rnorm(100) + 2*x\nplot(x,y)\ncor(x,y)\npoints(-2*x, 0.1*y, col='red') \ncor(-2*x, 0.1*y)\n\n0.906127948410664\n\n\n-0.906127948410664\n\n\n\n\n\n\n\n\n\n- 상관계수 : 기울기와 관련 X, 직선에 얼마나 몰려있는가"
  }
]